\documentclass{article}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

\usepackage{amsmath}
\usepackage{bbm}
\usepackage{siunitx}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{color}
\usepackage{textcomp}
\usepackage{graphicx}
\usepackage[margin=2.54cm]{geometry}
\usepackage{subcaption}
\usepackage[section]{placeins}

\definecolor{matlabgreen}{RGB}{28,172,0}
\definecolor{matlablilas}{RGB}{170,55,241}

\newcommand{\includecode}[1]{\lstinputlisting[caption={\ttfamily #1.m},
    label={lst:#1}]{matlab/#1.m}}
\newcommand{\inlinecode}[1]{\lstinline[basicstyle=\ttfamily,keywordstyle={}]{#1}}

\newcommand{\E}[1]{\operatorname{E}\left[#1\right]}
\newcommand{\Prob}[1]{\operatorname{P}\left[#1\right]}
\newcommand{\ProbC}[2]{\operatorname{P}\left[#1\middle|#2\right]}
%\newcommand{\calU}[0]{\mathcal{U}}
\newcommand{\ind}[1]{\operatorname{\mathbbm{1}}\left\{#1\right\}}

\newcolumntype{L}{>{$}l<{$}}

\author{Riccardo Zanol}
\title{Homework 1}

\begin{document}
\lstset{
  language=Matlab,
  basicstyle={\ttfamily \footnotesize},
  breaklines=true,
  morekeywords={true,false,warning,xlim,ylim},
  keywordstyle=\color{blue},
  stringstyle=\color{matlablilas},
  commentstyle={\color{matlabgreen} \itshape},
  numberstyle={\ttfamily \tiny},
  frame=leftline,
  showstringspaces=false,
  numbers=left,
  upquote=true,
}
\maketitle
\section*{1}
The replicas of figure 2.1 and 2.2 are shown as
Fig.~\ref{plot_2_1}~and~\ref{plot_2_2}. The histogram values are
computed with \inlinecode{histogram_1} and the ECDF with
\inlinecode{empirical_cdf}.
\begin{figure}[htbp]
  \centering
  \begin{subfigure}{0.5\textwidth}
    \centering
    \includegraphics[width=0.95\textwidth]{matlab/fig2_1}
    \caption{Raw data and histograms}
    \label{plot_2_1}
  \end{subfigure}%
  \begin{subfigure}{0.5\textwidth}
    \centering
    \includegraphics[width=0.7\textwidth]{matlab/fig2_2}
    \caption{ECDFs}
    \label{plot_2_2}
  \end{subfigure}
  \caption{Figures 2.1 and 2.2}
\end{figure}

The statistics that are needed to draw Fig.2.3 are shown in
Table~\ref{boxplot_table}, in this table the dispersion lower
bound is set to zero for both the new and the old execution times
because they cannot assume negative values. To compute the mean
confidence intervals the function \inlinecode{mean_ci} is used,
which uses the central limit theorem to approximate it. The median
CIs are computed by \inlinecode{median_ci} which applies the
Theorem~2.1. For the dispersion, the definition given in the
caption of Fig.~2.3 is used to computed it as 1.5 times the
interquartile distance. Theorem~2.5 is used to compute the
prediction intervals based on the order statistic in
\inlinecode{prediction_interval}.
\begin{table}[htbp]
  \centering
  \begin{tabular}{lLL}
    & \text{Old} & \text{New} \\
    \hline
    Mean & 60.162 & 34.100 \\
    Mean CI & [50.928, 69.395] & [29.262, 38.938] \\
    Median & 48.104 & 28.865 \\
    Median CI & [44.430, 55.873] & [27.813, 31.233] \\
    First quartile & 22.956 & 16.626 \\
    Third quartile & 94.786 & 45.626 \\
    Dispersion & [0, 202.530] & [0, 89.127] \\
    Prediction interval & [0.538, 182.308] & [1.511, 100.863] \\
  \end{tabular}
  \caption{Data for the box plot of the execution times of Fig.~2.3}
  \label{boxplot_table}
\end{table}

The difference between the old and the new execution times of
Figure 2.7 is plotted together with its histogram and box plot in
Fig.~\ref{diff_plots}. The mean of the difference, along with its
95\% confidence interval, is overlayed on the box plot by
\inlinecode{box_plot_with_mean}.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\textwidth]{matlab/diff_plots}
  \caption{Difference between the old and the new execution times}
  \label{diff_plots}
\end{figure}

Figure 2.8 is replicated in Fig.~\ref{cis_plot}, where the blue
bars represent the mean and the CI of the execution times
calculated assuming that the samples are Gaussian
(\inlinecode{mean_ci_gaussian}), the red bars are calculated like
in the general case using the central limit theorem, while the
green bars are calculated using the bootstrap technique
(\inlinecode{mean_ci_bootstrap}).
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.5\textwidth]{matlab/cis_plot}
  \caption{Confidence intervals calculated assuming normal data, in
    the general case and using bootstrap}
  \label{cis_plot}
\end{figure}

The plots of Example 2.5 are reproduced in
Figures~\ref{joe_data},~\ref{joe_autocorr}~and~\ref{joe_lagplot}. The
first one shows the values that the random variable $Y_t$ assumes
for $N = 93$ days. In the second plot there is the autocorrelation
$r_Y(n)$ of $Y_t$ (normalized with $r_Y(0)$) along with the 95\%
confidence interval $[-1.96/\sqrt{N}, 1.96/\sqrt{N}]$ that should
contain the estimated values of $r_Y(n>0)$ if the samples were
i.i.d., in this case we can see that they are very correlated when
the lag $n=1$.  In the last figure there are the lag plots of
$Y_t$ for lags $1\dots9$. In the plot with lag 1 a downward trend
can be seen, in agreement with the fact that $r_Y(1) \approx
-0.5$.
\begin{figure}[htbp]
  \centering
  \begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[width=0.95\textwidth]{matlab/joe_data}
    \caption{Raw data}
    \label{joe_data}
  \end{subfigure}%
  \begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[width=0.95\textwidth]{matlab/joe_autocorr}
    \caption{Autocorrelation}
    \label{joe_autocorr}
  \end{subfigure}
  \caption{Data of Fig.~2.10}
\end{figure}
\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{matlab/lagplot}
  \caption{Lag plots pf Fig.~2.10}
  \label{joe_lagplot}
\end{figure}

\section*{2}
The script \inlinecode{ex2.m} generates $K=1000$ times a vector of
$n=48$ uniform random variables, calculates the sample mean of each
vector with its CI and plots them sorted by lower extreme in
Fig.~\ref{uniform_mean_ci}. Since the number of samples $n$ in each
vector is big enough, the CIs are computed approximating the sample
mean r.v. with a Gaussian.

The confidence intervals don't contain the true value of the mean
$\mu=0.5$ in 52 cases out of $K$, but this is expected since, by
definition of CI, the probability that it contains $\mu$ is $\approx
5\%$.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.5\textwidth]{matlab/uniform_mean_ci}
  \caption{Confidence intervals for the mean of $n$ samples of a uniform r.v.}
  \label{uniform_mean_ci}
\end{figure}
The two lines in the plot have the shape of a Gaussian distribution
function and this tells that the approximation from the CLT used to
compute the CIs is good.
\section*{3}
The distribution function of the order statistic can be written as
\[ F_{U_{(k)}}(u) = \Prob{U_{(k)} \leq u} = \Prob{\sum_{j=1}^n\ind{U_j \leq u} \geq k} \]
which, using the total probability theorem, becomes
\begin{align*}
  & \sum_{i=0}^n\left(\ProbC{\sum_{j=1}^n\ind{U_j \leq u} \geq k}{\sum_{j=1}^n\ind{U_j \leq u} = i}\Prob{\sum_{j=1}^n\ind{U_j \leq u} = i}\right) = \\
  &= \sum_{i=k}^n\left( \Prob{\sum_{j=1}^n\ind{U_j \leq u} = i} \right) . 
\end{align*}
Since the random variable $\ind{U_j \leq u}$ is Bernoullian with
success probability $u$, the probability inside the sum is the
probability distribution of a binomial random variable with parameters
$n$ and $u$. So the distribution function of $U_{(k)}$
\[  F_{U_{(k)}}(u) = \sum_{i=k}^n\left(\binom{n}{i}u^i(1-u)^{n-i}\right)
= 1 - \sum_{i=0}^{k-1}\left(\binom{n}{i}u^i(1-u)^{n-i}\right) \]
is the complementary distribution function of a
$\mathrm{Bin}(n,u)$ evaluated at point $k$.

Now it is possible to directly compute the mean of $U_{(k)}$:
\[ \E{U_{(k)}} = \int_0^1 u \mathrm{d} F_{U_{(k)}}(u) \]
where the probability density function can be obtained by
differentiating the distribution function:
\[ \frac{\mathrm{d}}{\mathrm{d}u} F_{U_{(k)}}(u) =
- \sum_{i=0}^{k-1}\binom{n}{i}\left( iu^{i-1}(1-u)^{n-i} - u^i(n-i)(1-u)^{n-i-1} \right) \]
and then the integral and the sum can be swapped, since the sum is finite:
\[ \E{U_{(k)}} = -\sum_{i=0}^{k-1}\binom{n}{i}\left( i\int_0^1u^i(1-u)^{n-i}\mathrm{d}u - (n-i)\int_0^1u^{i+1}(1-u)^{n-i-1}\mathrm{d}u \right) . \]
Defining the function
\[ I(i) = \int_0^1u^i(1-u)^{n-i} \mathrm{d}u \]
the mean can be written as
\[ \E{U_{(k)}} = -\sum_{i=0}^{k-1}\binom{n}{i}\left( iI(i) - (n-i)I(i+1) \right) \]
and, since the solution of the integral $I(i)$ is
\[ I(i) = \frac{i!(n-i)!}{n!(n+1)} \]
the mean simplifies to
\[ \E{U_{(k)}} = \frac{k}{n+1} . \]
\section*{4}
The script \inlinecode{ex4.m} generates a vector of $n$ uniform random
variables for $n = 1,2\dots500$, computes the mean, the variance with
its CI and the prediction interval for each vector and plots
everything as $n$ varies. In Fig.~\ref{uniform_mean} there is the
sample mean of the vectors, that can be seen to converge to the true
value $\mu=0.5$ like $1/n$. In Fig.\ref{uniform_var} there is the plot
of the variances with their 95\% confidence intervals computed using
the bootstrap method and the true value of the variance $\sigma^2 =
\frac{1}{12}$. In this case the estimate converges more slowly to the
true value as $1/\sqrt{n}$ (and the size of the CIs also decreases at
the same rate).
\begin{figure}[htbp]
  \centering
  \begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[width=0.95\textwidth]{matlab/uniform_mean_n}
    \caption{Sample mean for $n$ uniform random variables}
    \label{uniform_mean}
    \end{subfigure}%
  \begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[width=0.95\textwidth]{matlab/uniform_variance_n}
    \caption{Sample variance for $n$ uniform random variables}
    \label{uniform_var}
  \end{subfigure}
  \begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[width=0.95\textwidth]{matlab/uniform_prediction_interval_n}
    \caption{Prediction interval for $n$ uniform random variables}
    \label{pred_int_unif}  
  \end{subfigure}
  \caption{Statistics for a vector of uniform random variables}
\end{figure}
The prediction interval, shown in Fig.~\ref{pred_int_unif}, does not
depend on $n$, so as soon as there are enough samples ($n=39$) it
remains around the same value. A theoretical 95\% prediction interval
for a uniform r.v. is just any interval of length 0.95 contained in
$[0,1]$. In the figure the interval $[0.025, 0.975]$ is compared to
the estimated one.
\section*{5}
For a vector of $n = 48$ Gaussian samples the means with CIs are
plotted in Fig.~\ref{gaussian_mean_ci}, this time the confidence
intervals are computed knowing that the samples are Gaussian so the
sample mean is distributed like a Student's~t. Also this time the
number of CIs that do not contain $\mu=0$ is $55/1000 \approx 5\%$.
The shape also remains the same since $n$ is big and the sample mean's
distribution could be approximated with a Gaussian.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.5\textwidth]{matlab/gaussian_mean_ci}
  \caption{Sample mean CI for $n$ Gaussian samples}
  \label{gaussian_mean_ci}
\end{figure}

Like in point 4 the mean (Fig.~\ref{gaussian_mean_n}), the variance
with CIs (Fig.\ref{gaussian_var_n}) and the prediction interval
(Fig.\ref{gaussian_pred_int_n}) are computed for a vector of $n$
Gaussian r.v.s with $n = 1,2\dots500$. This time, instead of using the
CLT, the sample mean is treated as having a Student's
t-distribution. So the confidence intervals for the mean and the
variance are computed using theorem 2.3, and the prediction interval
is computed using theorem 2.6 instead of the one for the general
i.i.d. case.
\begin{figure}[htbp]
  \centering
  \begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[width=0.95\textwidth]{matlab/normal_mean_n}
    \caption{Mean for $n$ Gaussian random variables}
    \label{gaussian_mean_n}
  \end{subfigure}%
  \begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[width=0.95\textwidth]{matlab/normal_variance_n}
    \caption{Variance for $n$ Gaussian random variables}
    \label{gaussian_var_n}
  \end{subfigure}
  \begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[width=0.95\textwidth]{matlab/normal_prediction_interval_n}
    \caption{Prediction interval for $n$ Gaussian random variables}
    \label{gaussian_pred_int_n}
  \end{subfigure}
\end{figure}

\end{document}
